{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate gain ratio\n",
    "def gain_ratio(Y, feature, level_entropy):\n",
    "    \n",
    "    # parameter 'feature' is the numpy array of feature, whose gain_ratio is being calculated to decide whether to split upon it or not\n",
    "    # paarameter 'Y' is output\n",
    "    # parameter 'level_entropy'  is entropy of the current level, which has called function 'gain_ratio' to decide which feature to split on\n",
    "    # variable 's_i' stores the value of split info \n",
    "    \n",
    "    s_i = split_info(feature)\n",
    "    \n",
    "    # variable N stores total number of sample points in numpya array of the feature\n",
    "    N = len(feature)\n",
    "    \n",
    "    # variable 'info_req' stores the value of weighed entropy in case decision tree splits on the feature\n",
    "    info_req = 0\n",
    "    \n",
    "    # variable 'feature_classes' is numpy array of all possible values in feature  \n",
    "    feature_classes = np.array(list(set(feature)))\n",
    "    for i in range(len(feature_classes)):\n",
    "        \n",
    "        # variable 'frac' stores the values of weight of a particular class in feature\n",
    "        frac = len(feature[feature == feature_classes[i]])/N\n",
    "        \n",
    "        # variable 'frac_entropy' stores the entropy/information requires of a paricular class in the feature\n",
    "        frac_entropy = entropy(Y[feature == feature_classes[i]])\n",
    "\n",
    "        info_req = info_req + frac * frac_entropy\n",
    "    \n",
    "\n",
    "    gain_ratio = (level_entropy - info_req)/s_i\n",
    "    \n",
    "    return(gain_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate split information\n",
    "def split_info(feature):\n",
    "    \n",
    "    # 'feature' is numpy array of a selected feature, whose split information is being claculated\n",
    "    \n",
    "    # variable 'N' stores total number sample points in feature\n",
    "    N = len(feature)\n",
    "    s_i = 0\n",
    "    feature_classes = np.array(list(set(feature)))\n",
    "    for i in range(len(feature_classes)):\n",
    "        frac = len(feature[feature == feature_classes[i]])/N\n",
    "        s_i = s_i - frac * math.log2(frac) \n",
    "        \n",
    "        if s_i == 0:\n",
    "            print(feature)\n",
    "        \n",
    "    return(s_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate entropy\n",
    "def entropy(Y):\n",
    "    \n",
    "    # 'Y' is numpy array of output\n",
    "    \n",
    "    # variable e stores value of entropy\n",
    "    e = 0\n",
    "    \n",
    "    # varaible 'n_Y' is total number of sample points in output 'Y'\n",
    "    n_Y = len(Y)\n",
    "    \n",
    "    # classes is numpy array which stores total number of possible values of output 'Y' \n",
    "    classes = np.array(list(set(Y)))\n",
    "    for i in range(len(classes)):\n",
    "\n",
    "        # variable 'n_class' stores total number of sample points belonging to a particular class\n",
    "        n_class = len(Y[Y == classes[i]])\n",
    "\n",
    "        # variable 'prob' stores probabality of occurance of a particular class in output 'Y'\n",
    "        prob = (n_class/n_Y)\n",
    "\n",
    "        e = e - prob * math.log2(prob) \n",
    "\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive function to implement decision tree\n",
    "def decision_tree(level,X,Y,Y_classes, features):\n",
    "    \n",
    "    # variable 'level' is an integer value of the level at which decision tree is further splitting\n",
    "    # variable 'X' is dictionary which has as feature names as keys, and numpy array of corresponding feature as values\n",
    "    # variable 'Y_classes' includes all possible values of poutput 'Y'\n",
    "    # variable 'Y' is numpy array of ouput\n",
    "    # variable 'feature' is numpy array of valid features, yet to be split upon\n",
    "    \n",
    "    print('Level', level)\n",
    "    \n",
    "    for i in range(len(Y_classes)):\n",
    "        print('Count of', Y_classes[i],'=',len(Y[Y == Y_classes[i]]))\n",
    "    \n",
    "    level_entropy = entropy(Y)\n",
    "    print('Current Entropy  is =',level_entropy)\n",
    "    \n",
    "    if level_entropy == 0:\n",
    "        print('Reached leaf Node')\n",
    "    elif len(features) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        \n",
    "        # variable 'max_gain_ratio' stores the value of maximum gain ratio amongst all features\n",
    "        max_gain_ratio = -1\n",
    "        \n",
    "        # variable 'feature_split' stores the feature, whose gain ratio is maximum on being split upon \n",
    "        feature_split = ''\n",
    "        \n",
    "        # loop for iterating over all valid features to find maximum gain ratio and corresponding feature\n",
    "        for i in range(len(features)):\n",
    "            feature_gain_ratio = gain_ratio(Y, X[features[i]], level_entropy)\n",
    "            if max_gain_ratio == -1:\n",
    "                max_gain_ratio = feature_gain_ratio\n",
    "                feature_split = features[i]\n",
    "            elif feature_gain_ratio > max_gain_ratio:\n",
    "                max_gain_ratio = feature_gain_ratio\n",
    "                feature_split = features[i]\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        print('Splitting on feature', feature_split ,'with gain ratio', max_gain_ratio)\n",
    "        \n",
    "\n",
    "        # variable 'feature_classes' includes all possible values of the selected feature to split upon \n",
    "        feature_classes = np.array(list(set(X[feature_split])))\n",
    "        \n",
    "        # loop to iterate over all classes of selected feature to split upon\n",
    "        for i in range(len(feature_classes)):\n",
    "            \n",
    "            # variable 'X_split' is a dictionary which includes feature names as keys and split features as its values\n",
    "            X_split = {}\n",
    "            \n",
    "            # loop to iterate over all features to split the features accarding to selected feature to split upon \n",
    "            for j in range(len(features)):\n",
    "                X_split[ features[j] ] = X[features[j]][X[feature_split] == feature_classes[i]]\n",
    "                \n",
    "            # variable 'Y_split' is the numpy array, which only includes output only corresponding to particular class of selected feature to split upon   \n",
    "            Y_split = Y[X[feature_split] == feature_classes[i]]\n",
    "            \n",
    "            # variable 'feature_split' is the numpy array of valid features (it does not include already split upon features), passed to the next level\n",
    "            features_split = np.delete(features, np.where(feature_split))\n",
    "            \n",
    "            print('')\n",
    "            decision_tree(level + 1, X_split ,Y_split ,Y_classes , features_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_to_class(cont_data):\n",
    "    cont_data = np.array(cont_data)\n",
    "    \n",
    "    class_data = np.zeros(cont_data.shape)\n",
    "    \n",
    "    for i in range(cont_data.shape[0]):\n",
    "        for j in range(cont_data.shape[1]):\n",
    "\n",
    "            if cont_data[i,j] < np.mean(cont_data[:,j]):\n",
    "                class_data[i,j] = 0\n",
    "            else:\n",
    "                class_data[i,j] = 1\n",
    "    return class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 0\n",
      "Count of 0 = 50\n",
      "Count of 1 = 50\n",
      "Count of 2 = 50\n",
      "Current Entropy  is = 1.584962500721156\n",
      "Splitting on feature petal length (cm) with gain ratio 0.7967247093242663\n",
      "\n",
      "Level 1\n",
      "Count of 0 = 50\n",
      "Count of 1 = 7\n",
      "Count of 2 = 0\n",
      "Current Entropy  is = 0.5373760853377336\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4ccde9c5ca7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# calling decision tree function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdecision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-bb20e3ee2113>\u001b[0m in \u001b[0;36mdecision_tree\u001b[1;34m(level, X, Y, Y_classes, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mdecision_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_split\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mY_split\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mY_classes\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mfeatures_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-bb20e3ee2113>\u001b[0m in \u001b[0;36mdecision_tree\u001b[1;34m(level, X, Y, Y_classes, features)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# loop for iterating over all valid features to find maximum gain ratio and corresponding feature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0mfeature_gain_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgain_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel_entropy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_gain_ratio\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mmax_gain_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_gain_ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-51bd6d196e3d>\u001b[0m in \u001b[0;36mgain_ratio\u001b[1;34m(Y, feature, level_entropy)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mgain_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlevel_entropy\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0minfo_req\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0ms_i\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgain_ratio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "features = iris['feature_names']\n",
    "\n",
    "\n",
    "class_data = cont_to_class(iris['data'])\n",
    "# print(class_data)\n",
    "\n",
    "X = {}\n",
    "\n",
    "for i in range(len(iris['feature_names'])):\n",
    "    X[iris['feature_names'][i]] = np.array(class_data[:,i])\n",
    "    \n",
    "    \n",
    "Y = iris['target']\n",
    "\n",
    "Y_classes = np.array(list(set(Y)))\n",
    "\n",
    "\n",
    "# calling decision tree function\n",
    "decision_tree(0,X,Y,Y_classes, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
